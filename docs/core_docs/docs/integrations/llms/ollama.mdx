# Ollama

:::caution
You are currently on a page documenting the use of Ollama models as [text completion models](/docs/concepts/#llms). Many popular models available on Ollama are [chat completion models](/docs/concepts/#chat-models).

You may be looking for [this page instead](/docs/integrations/chat/ollama/).
:::

[Ollama](https://ollama.ai/) allows you to run open-source large language models, such as Llama 3, locally.

Ollama bundles model weights, configuration, and data into a single package, defined by a Modelfile. It optimizes setup and configuration details, including GPU usage.

This example goes over how to use LangChain to interact with an Ollama-run Llama 2 7b instance.
For a complete list of supported models and model variants, see the [Ollama model library](https://github.com/jmorganca/ollama#model-library).

## Setup

Follow [these instructions](https://github.com/jmorganca/ollama) to set up and run a local Ollama instance.

## Usage

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/community
```

import CodeBlock from "@theme/CodeBlock";
import OllamaExample from "@examples/models/llm/ollama.ts";

<CodeBlock language="typescript">{OllamaExample}</CodeBlock>

## Multimodal models

Ollama supports open source multimodal models like [LLaVA](https://ollama.ai/library/llava) in versions 0.1.15 and up.
You can bind base64 encoded image data to multimodal-capable models to use as context like this:

import OllamaMultimodalExample from "@examples/models/llm/ollama_multimodal.ts";

<CodeBlock language="typescript">{OllamaMultimodalExample}</CodeBlock>
