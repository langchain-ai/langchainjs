---
sidebar_label: Bedrock Converse
---

# ChatBedrockConverse

> [Amazon Bedrock Converse](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html) is a fully managed service that makes Foundation Models (FMs)
> from leading AI startups and Amazon available via an API. You can choose from a wide range of FMs to find the model that is best suited for your use case.

## Setup

You'll need to install the `@langchain/aws` package:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/aws
```

## Usage

import UnifiedModelParamsTooltip from "@mdx_components/unified_model_params_tooltip.mdx";

<UnifiedModelParamsTooltip></UnifiedModelParamsTooltip>

import CodeBlock from "@theme/CodeBlock";
import BedrockConverseExample from "@examples/models/chat/integration_bedrock_converse.ts";

<CodeBlock language="typescript">{BedrockConverseExample}</CodeBlock>

:::tip
See the LangSmith traces for the above example [here](https://smith.langchain.com/public/7aeb0c56-9afa-441d-8659-4d52c007eae0/r), and [here for steaming](https://smith.langchain.com/public/74606291-45cd-478c-a874-568b2905427f/r).
:::

## Multimodal inputs

:::tip
Multimodal inputs are currently only supported by Anthropic Claude-3 models.
:::

Anthropic Claude-3 models hosted on Bedrock have multimodal capabilities and can reason about images. Here's an example:

import BedrockMultimodalExample from "@examples/models/chat/integration_bedrock_multimodal_converse.ts";

<CodeBlock language="typescript">{BedrockMultimodalExample}</CodeBlock>

:::tip
See the LangSmith trace [here](https://smith.langchain.com/public/c40f8d09-123a-4b3b-934a-625d5ee0f57a/r).
:::

## Tool calling

The examples below demonstrate how to use tool calling, along with the `withStructuredOutput` method to easily compose structured output LLM calls.

import ToolCalling from "@examples/models/chat/integration_bedrock_tools_converse.ts";

<CodeBlock language="typescript">{ToolCalling}</CodeBlock>

Check out the output of this tool call! We can see here it's using chain-of-thought before calling the tool, where it describes what it's going to do in plain text before calling the tool: `Okay, let's get the weather for New York City.`.

:::tip
See the LangSmith trace [here](https://smith.langchain.com/public/d34e378d-5044-4b5b-9ed7-3d2486fe5d47/r)
:::

### `.withStructuredOutput({ ... })`

Using the `.withStructuredOutput` method, you can easily make the LLM return structured output, given only a Zod or JSON schema:

import WSOExample from "@examples/models/chat/integration_bedrock_wso_converse.ts";

<CodeBlock language="typescript">{WSOExample}</CodeBlock>

:::tip
See the LangSmith trace [here](https://smith.langchain.com/public/982940c4-5f96-4168-80c9-99102c3e073a/r)
:::


## Related

- Chat model [conceptual guide](/docs/concepts/#chat-models)
- Chat model [how-to guides](/docs/how_to/#chat-models)
