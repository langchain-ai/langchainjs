---
sidebar_label: DeepSeek
title: ChatDeepSeek
---

export const quartoRawHtml = [
  `
<table style="width:100%;">
<colgroup>
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 15%" />
<col style="width: 15%" />
<col style="width: 15%" />
<col style="width: 15%" />
<col style="width: 15%" />
</colgroup>
<thead>
<tr>
<th style="text-align: left;">Class</th>
<th style="text-align: left;">Package</th>
<th style="text-align: center;">Local</th>
<th style="text-align: center;">Serializable</th>
<th style="text-align: center;"><a href="https://python.langchain.com/docs/integrations/chat/deepseek">PY support</a></th>
<th style="text-align: center;">Package downloads</th>
<th style="text-align: center;">Package latest</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><a href="https://api.js.langchain.com/classes/_langchain_deepseek.ChatDeepSeek.html"><code>ChatDeepSeek</code></a></td>
<td style="text-align: left;"><a href="https://npmjs.com/@langchain/deepseek"><code>@langchain/deepseek</code></a></td>
<td style="text-align: center;">❌ (see <a href="../../../docs/integrations/chat/ollama">Ollama</a>)</td>
<td style="text-align: center;">beta</td>
<td style="text-align: center;">✅</td>
<td style="text-align: center;"><img src="https://img.shields.io/npm/dm/@langchain/deepseek?style=flat-square&amp;label=%20&amp;.png" alt="NPM - Downloads" /></td>
<td style="text-align: center;"><img src="https://img.shields.io/npm/v/@langchain/deepseek?style=flat-square&amp;label=%20&amp;.png" alt="NPM - Version" /></td>
</tr>
</tbody>
</table>
`,
  `
<table style="width:100%;">
<colgroup>
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 11%" />
</colgroup>
<thead>
<tr>
<th style="text-align: center;"><a href="../../../docs/how_to/tool_calling">Tool calling</a></th>
<th style="text-align: center;"><a href="../../../docs/how_to/structured_output/">Structured output</a></th>
<th style="text-align: center;">JSON mode</th>
<th style="text-align: center;"><a href="../../../docs/how_to/multimodal_inputs/">Image input</a></th>
<th style="text-align: center;">Audio input</th>
<th style="text-align: center;">Video input</th>
<th style="text-align: center;"><a href="../../../docs/how_to/chat_streaming/">Token-level streaming</a></th>
<th style="text-align: center;"><a href="../../../docs/how_to/chat_token_usage_tracking/">Token usage</a></th>
<th style="text-align: center;"><a href="../../../docs/how_to/logprobs/">Logprobs</a></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">✅</td>
<td style="text-align: center;">✅</td>
<td style="text-align: center;">✅</td>
<td style="text-align: center;">❌</td>
<td style="text-align: center;">❌</td>
<td style="text-align: center;">❌</td>
<td style="text-align: center;">✅</td>
<td style="text-align: center;">✅</td>
<td style="text-align: center;">✅</td>
</tr>
</tbody>
</table>
`,
  `<!-- ## Invocation -->`,
];

This will help you getting started with DeepSeek [chat
models](../../../docs/concepts/#chat-models). For detailed documentation
of all `ChatDeepSeek` features and configurations head to the [API
reference](https://api.js.langchain.com/classes/_langchain_deepseek.ChatDeepSeek.html).

## Overview

### Integration details

<div dangerouslySetInnerHTML={{ __html: quartoRawHtml[0] }} />

### Model features

See the links in the table headers below for guides on how to use
specific features.

<div dangerouslySetInnerHTML={{ __html: quartoRawHtml[1] }} />

Note that as of 1/27/25, tool calling and structured output are not
currently supported for `deepseek-reasoner`.

## Setup

To access DeepSeek models you’ll need to create a DeepSeek account, get
an API key, and install the `@langchain/deepseek` integration package.

You can also access the DeepSeek API through providers like [Together
AI](../../../docs/integrations/chat/togetherai) or
[Ollama](../../../docs/integrations/chat/ollama).

### Credentials

Head to https://deepseek.com/ to sign up to DeepSeek and generate an API
key. Once you’ve done this set the `DEEPSEEK_API_KEY` environment
variable:

```bash
export DEEPSEEK_API_KEY="your-api-key"
```

If you want to get automated tracing of your model calls you can also
set your [LangSmith](https://docs.smith.langchain.com/) API key by
uncommenting below:

```bash
# export LANGSMITH_TRACING="true"
# export LANGSMITH_API_KEY="your-api-key"
```

### Installation

The LangChain ChatDeepSeek integration lives in the
`@langchain/deepseek` package:

```mdx-code-block
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";
import Npm2Yarn from "@theme/Npm2Yarn";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

<Npm2Yarn>
  @langchain/deepseek @langchain/core
</Npm2Yarn>

```

## Instantiation

Now we can instantiate our model object and generate chat completions:

```typescript
import { ChatDeepSeek } from "@langchain/deepseek";

const llm = new ChatDeepSeek({
  model: "deepseek-reasoner",
  temperature: 0,
  // other params...
});
```

<div dangerouslySetInnerHTML={{ __html: quartoRawHtml[2] }} />

```typescript
const aiMsg = await llm.invoke([
  [
    "system",
    "You are a helpful assistant that translates English to French. Translate the user sentence.",
  ],
  ["human", "I love programming."],
]);
aiMsg;
```

```text
AIMessage {
  "id": "e2874482-68a7-4552-8154-b6a245bab429",
  "content": "J'adore la programmation.",
  "additional_kwargs": {,
    "reasoning_content": "...",
  },
  "response_metadata": {
    "tokenUsage": {
      "promptTokens": 23,
      "completionTokens": 7,
      "totalTokens": 30
    },
    "finish_reason": "stop",
    "model_name": "deepseek-reasoner",
    "usage": {
      "prompt_tokens": 23,
      "completion_tokens": 7,
      "total_tokens": 30,
      "prompt_tokens_details": {
        "cached_tokens": 0
      },
      "prompt_cache_hit_tokens": 0,
      "prompt_cache_miss_tokens": 23
    },
    "system_fingerprint": "fp_3a5770e1b4"
  },
  "tool_calls": [],
  "invalid_tool_calls": [],
  "usage_metadata": {
    "output_tokens": 7,
    "input_tokens": 23,
    "total_tokens": 30,
    "input_token_details": {
      "cache_read": 0
    },
    "output_token_details": {}
  }
}
```

```typescript
console.log(aiMsg.content);
```

```text
J'adore la programmation.
```

## Chaining

We can [chain](../../../docs/how_to/sequence/) our model with a prompt
template like so:

```typescript
import { ChatPromptTemplate } from "@langchain/core/prompts";

const prompt = ChatPromptTemplate.fromMessages([
  [
    "system",
    "You are a helpful assistant that translates {input_language} to {output_language}.",
  ],
  ["human", "{input}"],
]);

const chain = prompt.pipe(llm);
await chain.invoke({
  input_language: "English",
  output_language: "German",
  input: "I love programming.",
});
```

```text
AIMessage {
  "id": "6e7f6f8c-8d7a-4dad-be07-425384038fd4",
  "content": "Ich liebe es zu programmieren.",
  "additional_kwargs": {,
    "reasoning_content": "...",
  },
  "response_metadata": {
    "tokenUsage": {
      "promptTokens": 18,
      "completionTokens": 9,
      "totalTokens": 27
    },
    "finish_reason": "stop",
    "model_name": "deepseek-reasoner",
    "usage": {
      "prompt_tokens": 18,
      "completion_tokens": 9,
      "total_tokens": 27,
      "prompt_tokens_details": {
        "cached_tokens": 0
      },
      "prompt_cache_hit_tokens": 0,
      "prompt_cache_miss_tokens": 18
    },
    "system_fingerprint": "fp_3a5770e1b4"
  },
  "tool_calls": [],
  "invalid_tool_calls": [],
  "usage_metadata": {
    "output_tokens": 9,
    "input_tokens": 18,
    "total_tokens": 27,
    "input_token_details": {
      "cache_read": 0
    },
    "output_token_details": {}
  }
}
```

## API reference

For detailed documentation of all ChatDeepSeek features and
configurations head to the API reference:
https://api.js.langchain.com/classes/\_langchain_deepseek.ChatDeepSeek.html


## Related

- Chat model [conceptual guide](/docs/concepts/#chat-models)
- Chat model [how-to guides](/docs/how_to/#chat-models)
