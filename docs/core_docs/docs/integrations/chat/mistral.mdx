---
sidebar_label: Mistral AI
---

import CodeBlock from "@theme/CodeBlock";

# ChatMistralAI

[Mistral AI](https://mistral.ai/) is a research organization and hosting platform for LLMs.
The LangChain implementation of Mistral's models uses their hosted generation API, making it easier to access their models without needing to run them locally.

:::tip
Want to run Mistral's models locally? Check out our [Ollama integration](/docs/integrations/chat/ollama).
:::

## Models

Mistral's API offers access to two of their open source, and proprietary models.
See [this page](https://docs.mistral.ai/getting-started/models/) for an up to date list.

## Setup

In order to use the Mistral API you'll need an API key. You can sign up for a Mistral account and create an API key [here](https://console.mistral.ai/).

You'll first need to install the [`@langchain/mistralai`](https://www.npmjs.com/package/@langchain/mistralai) package:

import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/mistralai
```

import UnifiedModelParamsTooltip from "@mdx_components/unified_model_params_tooltip.mdx";

<UnifiedModelParamsTooltip></UnifiedModelParamsTooltip>

## Usage

When sending chat messages to mistral, there are a few requirements to follow:

- The first message can _*not*_ be an assistant (ai) message.
- Messages _*must*_ alternate between user and assistant (ai) messages.
- Messages can _*not*_ end with an assistant (ai) or system message.

import ChatMistralAIExample from "@examples/models/chat/chat_mistralai.ts";

<CodeBlock language="typescript">{ChatMistralAIExample}</CodeBlock>

:::info
You can see a LangSmith trace of this example [here](https://smith.langchain.com/public/d69d0db9-f29e-45aa-a40d-b53f6273d7d0/r)
:::

### Streaming

Mistral's API also supports streaming token responses. The example below demonstrates how to use this feature.

import ChatStreamMistralAIExample from "@examples/models/chat/chat_stream_mistralai.ts";

<CodeBlock language="typescript">{ChatStreamMistralAIExample}</CodeBlock>

:::info
You can see a LangSmith trace of this example [here](https://smith.langchain.com/public/061d90f2-ac7e-44c5-8790-8b23299f9217/r)
:::

### Tool calling

Mistral's API now supports tool calling and JSON mode!
The examples below demonstrates how to use them, along with how to use the `withStructuredOutput` method to easily compose structured output LLM calls.

import ToolCalling from "@examples/models/chat/chat_mistralai_tools.ts";

<CodeBlock language="typescript">{ToolCalling}</CodeBlock>

### `.withStructuredOutput({ ... })`

:::info
The `.withStructuredOutput` method is in beta. It is actively being worked on, so the API may change.
:::

Using the `.withStructuredOutput` method, you can easily make the LLM return structured output, given only a Zod or JSON schema:

:::note
The Mistral tool calling API requires descriptions for each tool field. If descriptions are not supplied, the API will error.
:::

import WSAExample from "@examples/models/chat/chat_mistralai_wsa.ts";

<CodeBlock language="typescript">{WSAExample}</CodeBlock>

### Using JSON schema:

import WSAJSONExample from "@examples/models/chat/chat_mistralai_wsa_json.ts";

<CodeBlock language="typescript">{WSAJSONExample}</CodeBlock>

### Tool calling agent

The larger Mistral models not only support tool calling, but can also be used in the Tool Calling agent.
Here's an example:

import AgentsExample from "@examples/models/chat/chat_mistralai_agents.ts";

<CodeBlock language="typescript">{AgentsExample}</CodeBlock>
