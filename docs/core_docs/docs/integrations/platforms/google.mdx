---
keywords: [gemini, gemini-pro]
---

# Google

Functionality related to [Google Cloud Platform](https://cloud.google.com/)

## Chat models

### Gemini Models

Access Gemini models such as `gemini-1.5-pro` and `gemini-1.5-flex` through the [`ChatGoogleGenerativeAI`](/docs/integrations/chat/google_generativeai),
or if using VertexAI, via the [`ChatVertexAI`](/docs/integrations/chat/google_vertex_ai) class.

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import IntegrationInstallTooltip from "@mdx_components/integration_install_tooltip.mdx";

<Tabs>
<TabItem value="genai" label="GenAI" default>
<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/google-genai @langchain/core
```

Configure your API key.

```
export GOOGLE_API_KEY=your-api-key
```

```typescript
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";

const model = new ChatGoogleGenerativeAI({
  model: "gemini-pro",
  maxOutputTokens: 2048,
});

// Batch and stream are also supported
const res = await model.invoke([
  [
    "human",
    "What would be a good company name for a company that makes colorful socks?",
  ],
]);
```

Gemini vision models support image inputs when providing a single human message. For example:

```typescript
const visionModel = new ChatGoogleGenerativeAI({
  model: "gemini-pro-vision",
  maxOutputTokens: 2048,
});
const image = fs.readFileSync("./hotdog.jpg").toString("base64");
const input2 = [
  new HumanMessage({
    content: [
      {
        type: "text",
        text: "Describe the following image.",
      },
      {
        type: "image_url",
        image_url: `data:image/png;base64,${image}`,
      },
    ],
  }),
];

const res = await visionModel.invoke(input2);
```

:::tip
Click [here](/docs/integrations/chat/google_generativeai) for the `@langchain/google-genai` specific integration docs
:::

</TabItem>

<TabItem value="vertexai" label="VertexAI" default>
<IntegrationInstallTooltip></IntegrationInstallTooltip>

```bash npm2yarn
npm install @langchain/google-vertexai @langchain/core
```

Then, you'll need to add your service account credentials, either directly as a `GOOGLE_VERTEX_AI_WEB_CREDENTIALS` environment variable:

```
GOOGLE_VERTEX_AI_WEB_CREDENTIALS={"type":"service_account","project_id":"YOUR_PROJECT-12345",...}
```

or as a file path:

```
GOOGLE_VERTEX_AI_WEB_CREDENTIALS_FILE=/path/to/your/credentials.json
```

```typescript
import { ChatVertexAI } from "@langchain/google-vertexai";
// Or, if using the web entrypoint:
// import { ChatVertexAI } from "@langchain/google-vertexai-web";

const model = new ChatVertexAI({
  model: "gemini-1.0-pro",
  maxOutputTokens: 2048,
});

// Batch and stream are also supported
const res = await model.invoke([
  [
    "human",
    "What would be a good company name for a company that makes colorful socks?",
  ],
]);
```

Gemini vision models support image inputs when providing a single human message. For example:

```typescript
const visionModel = new ChatVertexAI({
  model: "gemini-pro-vision",
  maxOutputTokens: 2048,
});
const image = fs.readFileSync("./hotdog.png").toString("base64");
const input2 = [
  new HumanMessage({
    content: [
      {
        type: "text",
        text: "Describe the following image.",
      },
      {
        type: "image_url",
        image_url: `data:image/png;base64,${image}`,
      },
    ],
  }),
];

const res = await visionModel.invoke(input2);
```

:::tip
Click [here](/docs/integrations/chat/google_vertex_ai) for the `@langchain/google-vertexai` specific integration docs
:::

</TabItem>
</Tabs>

The value of `image_url` must be a base64 encoded image (e.g., `data:image/png;base64,abcd124`).

### Non-Gemini Models

See above for setting up authentication through Vertex AI to use these models.

[Anthropic](/docs/integrations/chat/anthropic) Claude models are also available through
the [Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude)
platform. See [here](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude)
for more information about enabling access to the models and the model names to use.

PaLM models are no longer supported.

## Vector Store

### Vertex AI Vector Search

> [Vertex AI Vector Search](https://cloud.google.com/vertex-ai/docs/matching-engine/overview),
> formerly known as Vertex AI Matching Engine, provides the industry's leading high-scale
> low latency vector database. These vector databases are commonly
> referred to as vector similarity-matching or an approximate nearest neighbor (ANN) service.

```typescript
import { MatchingEngine } from "langchain/vectorstores/googlevertexai";
```

## Tools

### Google Search

- Set up a Custom Search Engine, following [these instructions](https://stackoverflow.com/questions/37083058/programmatically-searching-google-in-python-using-custom-search)
- Get an API Key and Custom Search Engine ID from the previous step, and set them as environment variables `GOOGLE_API_KEY` and `GOOGLE_CSE_ID` respectively

There exists a `GoogleCustomSearch` utility which wraps this API. To import this utility:

```typescript
import { GoogleCustomSearch } from "langchain/tools";
```

We can easily load this wrapper as a Tool (to use with an Agent). We can do this with:

```typescript
const tools = [new GoogleCustomSearch({})];
// Pass this variable into your agent.
```
