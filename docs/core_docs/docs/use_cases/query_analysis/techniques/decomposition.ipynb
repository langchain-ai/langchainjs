{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a47da0d0-0927-4adb-93e6-99a434f732cf",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2195672-0cab-4967-ba8a-c6544635547d",
   "metadata": {},
   "source": [
    "# Decomposition\n",
    "\n",
    "When a user asks a question there is no guarantee that the relevant results can be returned with a single query. Sometimes to answer a question we need to split it into distinct sub-questions, retrieve results for each sub-question, and then answer using the cumulative context.\n",
    "\n",
    "For example if a user asks: \"How is Web Voyager different from reflection agents\", and we have one document that explains Web Voyager and one that explains reflection agents but no document that compares the two, then we'd likely get better results by retrieving for both \"What is Web Voyager\" and \"What are reflection agents\" and combining the retrieved documents than by retrieving based on the user question directly.\n",
    "\n",
    "This process of splitting an input into multiple distinct sub-queries is what we refer to as **query decomposition**. It is also sometimes referred to as sub-query generation. In this guide we'll walk through an example of how to do decomposition, using our example of a Q&A bot over the LangChain YouTube videos from the [Quickstart](/docs/use_cases/query_analysis/quickstart)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4079b57-4369-49c9-b2ad-c809b5408d7e",
   "metadata": {},
   "source": [
    "## Setup\n",
    "#### Install dependencies\n",
    "\n",
    "```{=mdx}\n",
    "import IntegrationInstallTooltip from \"@mdx_components/integration_install_tooltip.mdx\";\n",
    "import Npm2Yarn from \"@theme/Npm2Yarn\";\n",
    "\n",
    "<IntegrationInstallTooltip></IntegrationInstallTooltip>\n",
    "\n",
    "<Npm2Yarn>\n",
    "  @langchain/core zod uuid\n",
    "</Npm2Yarn>\n",
    "```\n",
    "\n",
    "#### Set environment variables\n",
    "\n",
    "```\n",
    "# Optional, use LangSmith for best-in-class observability\n",
    "LANGSMITH_API_KEY=your-api-key\n",
    "LANGCHAIN_TRACING_V2=true\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57396e23-c192-4d97-846b-5eacea4d6b8d",
   "metadata": {},
   "source": [
    "## Query generation\n",
    "\n",
    "To convert user questions to a list of sub questions we'll use a LLM function-calling API, which can return multiple functions each turn:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf59700",
   "metadata": {},
   "source": [
    "```{=mdx}\n",
    "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
    "\n",
    "<ChatModelTabs customVarName=\"llm\" />\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b51dd76-820d-41a4-98c8-893f6fe0d1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { z } from \"zod\";\n",
    "\n",
    "const subQuerySchema = z.object({\n",
    "  subQuery: z.array(z.string().describe(\"A very specific query against the database\"))\n",
    "}).describe(\"Search over a database of tutorial videos about a software library\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "783c03c3-8c72-4f88-9cf4-5829ce6745d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatPromptTemplate, MessagesPlaceholder } from \"@langchain/core/prompts\";\n",
    "\n",
    "const system = `You are an expert at converting user questions into database queries.\n",
    "You have access to a database of tutorial videos about a software library for building LLM-powered applications.\n",
    "\n",
    "Perform query decomposition. Given a user question, break it down into distinct sub questions that\n",
    "you need to answer in order to answer the original question.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\n",
    "If the query is already well formed, do not try to decompose it further.`;\n",
    "const prompt = ChatPromptTemplate.fromMessages(\n",
    "  [\n",
    "    [\"system\", system],\n",
    "    new MessagesPlaceholder({\n",
    "      variableName: \"examples\",\n",
    "      optional: true,\n",
    "    }),\n",
    "    [\"human\", \"{question}\"],\n",
    "  ]\n",
    ")\n",
    "const llmWithTools = llm.withStructuredOutput(subQuerySchema, {\n",
    "  name: \"SubQuery\"\n",
    "})\n",
    "const queryAnalyzer = prompt.pipe(llmWithTools);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f403517a-b8e3-44ac-b0a6-02f8305635a2",
   "metadata": {},
   "source": [
    "Let's try it out with a simple question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92bc7bac-700d-4666-b523-f0f8c3644ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ subQuery: [ \u001b[32m\"How to do rag\"\u001b[39m ] }"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await queryAnalyzer.invoke({ \"question\": \"how to do rag\" })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7075a94",
   "metadata": {},
   "source": [
    "Now with two slightly more involved questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87590c6d-edd7-4805-bf68-c906907f9291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  subQuery: [\n",
       "    \u001b[32m\"How to use multi-modal models in a chain\"\u001b[39m,\n",
       "    \u001b[32m\"How to turn a chain into a REST API\"\u001b[39m\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await queryAnalyzer.invoke(\n",
    "    {\n",
    "        \"question\": \"how to use multi-modal models in a chain and turn chain into a rest api\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c949da8-b97e-45f5-937b-5c431e59edad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  subQuery: [\n",
       "    \u001b[32m\"Difference between Web Voyager and Reflection Agents\"\u001b[39m,\n",
       "    \u001b[32m\"Do Web Voyager and Reflection Agents use LangGraph?\"\u001b[39m\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await queryAnalyzer.invoke(\n",
    "    {\n",
    "        \"question\": \"what's the difference between web voyager and reflection agents? do they use langgraph?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ba81f0-f00b-4656-b840-037bf4306c60",
   "metadata": {},
   "source": [
    "## Adding examples and tuning the prompt\n",
    "\n",
    "This works pretty well, but we probably want it to decompose the last question even further to separate the queries about Web Voyager and Reflection Agents. If we aren't sure up front what types of queries will do best with our index, we can also intentionally include some redundancy in our queries, so that we return both sub queries and higher level queries. \n",
    "\n",
    "To tune our query generation results, we can add some examples of inputs questions and gold standard output queries to our prompt. We can also try to improve our system message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d00d74b-7bc7-4224-ad09-fff8e7aeeaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "const examples: Array<Record<string, any>> = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "171f3c37-36da-4a80-911e-d0447168b9d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[33m1\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const question = \"What's chat langchain, is it a langchain template?\"\n",
    "const query = {\n",
    "    query: \"What's chat langchain, is it a langchain template?\",\n",
    "    subQueries: [\n",
    "        \"What is chat langchain\",\n",
    "        \"Is chat langchain a langchain template\"\n",
    "    ]\n",
    "}\n",
    "examples.push({ \"input\": question, \"toolCalls\": [query] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92523941-5aa0-4d1e-a795-1d14529b48c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[33m2\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const question = \"How would I use LangGraph to build an automaton\"\n",
    "const query = {\n",
    "    query: \"How would I use LangGraph to build an automaton\",\n",
    "    subQueries: [\n",
    "        \"How to build automaton with LangGraph\"\n",
    "    ]\n",
    "}\n",
    "examples.push({\"input\": question, \"toolCalls\": [query] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "844df58a-abd3-4c06-9a59-b7eccbbefc0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[33m3\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const question = \"How to build multi-agent system and stream intermediate steps from it\"\n",
    "const query = {\n",
    "    query: \"How to build multi-agent system and stream intermediate steps from it\",\n",
    "    subQueries: [\n",
    "        \"How to build multi-agent system\",\n",
    "        \"How to stream intermediate steps\",\n",
    "        \"How to stream intermediate steps from multi-agent system\",\n",
    "    ]\n",
    "}\n",
    "examples.push({\"input\": question, \"toolCalls\": [query] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45288345-0c5a-4c57-b007-8981ce21aedd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[33m4\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const question = \"What's the difference between LangChain agents and LangGraph?\"\n",
    "const query = {\n",
    "    query: \"What's the difference between LangChain agents and LangGraph?\",\n",
    "    subQueries: [\n",
    "        \"What's the difference between LangChain agents and LangGraph?\",\n",
    "        \"What are LangChain agents\",\n",
    "        \"What is LangGraph\",\n",
    "    ]\n",
    "}\n",
    "examples.push({\"input\": question, \"toolCalls\": [query] })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68ee464-9f72-4a76-96fb-a87aeb29daa3",
   "metadata": {},
   "source": [
    "Now we need to update our prompt template and chain so that the examples are included in each prompt. Since we're working with LLM model function-calling, we'll need to do a bit of extra structuring to send example inputs and outputs to the model. We'll create a `toolExampleToMessages` helper function to handle this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80a33517-afa5-4152-a041-55e01eadf04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { v4 as uuidV4 } from \"uuid\";\n",
    "import {\n",
    "  AIMessage,\n",
    "  BaseMessage,\n",
    "  HumanMessage,\n",
    "  SystemMessage,\n",
    "  ToolMessage,\n",
    "} from \"@langchain/core/messages\";\n",
    "\n",
    "const toolExampleToMessages = (example: Record<string, any>): Array<BaseMessage> => {\n",
    "  const messages: Array<BaseMessage> = [new HumanMessage({ content: example.input })];\n",
    "  const openaiToolCalls = example.toolCalls.map((toolCall) => {\n",
    "    return {\n",
    "      id: uuidV4(),\n",
    "      type: \"function\" as const,\n",
    "      function: {\n",
    "        name: \"SubQuery\",\n",
    "        arguments: JSON.stringify(toolCall),\n",
    "      },\n",
    "    };\n",
    "  });\n",
    "\n",
    "  messages.push(new AIMessage({ content: \"\", additional_kwargs: { tool_calls: openaiToolCalls } }));\n",
    "\n",
    "  const toolOutputs = \"toolOutputs\" in example ? example.toolOutputs : Array(openaiToolCalls.length).fill(\"This is an example of a correct usage of this tool. Make sure to continue using the tool this way.\");\n",
    "  toolOutputs.forEach((output, index) => {\n",
    "    messages.push(new ToolMessage({ content: output, tool_call_id: openaiToolCalls[index].id }));\n",
    "  });\n",
    "\n",
    "  return messages;\n",
    "};\n",
    "\n",
    "const exampleMessages = examples.map((ex) => toolExampleToMessages(ex)).flat();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "696f4bf1-467a-497a-8478-9bc6c84dda33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { MessagesPlaceholder } from \"@langchain/core/prompts\";\n",
    "import { RunnablePassthrough, RunnableSequence } from \"@langchain/core/runnables\";\n",
    "\n",
    "const system = `You are an expert at converting user questions into database queries.\n",
    "You have access to a database of tutorial videos about a software library for building LLM-powered applications.\n",
    "\n",
    "Perform query decomposition. Given a user question, break it down into the most specific sub questions you can\n",
    "which will help you answer the original question. Each sub question should be about a single concept/fact/idea.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.`;\n",
    "const prompt = ChatPromptTemplate.fromMessages(\n",
    "  [\n",
    "    [\"system\", system],\n",
    "    new MessagesPlaceholder({ variableName: \"examples\", optional: true }),\n",
    "    [\"human\", \"{question}\"],\n",
    "  ]\n",
    ")\n",
    "const queryAnalyzerWithExamples = RunnableSequence.from([\n",
    "  {\n",
    "    question: new RunnablePassthrough(),\n",
    "    examples: () => exampleMessages,\n",
    "  },\n",
    "  prompt,\n",
    "  llmWithTools,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82824a2b-8985-430c-817a-6c8466bddf37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  query: \u001b[32m\"what's the difference between web voyager and reflection agents? do they use langgraph?\"\u001b[39m,\n",
       "  subQueries: [\n",
       "    \u001b[32m\"What's the difference between web voyager and reflection agents\"\u001b[39m,\n",
       "    \u001b[32m\"Do web voyager and reflection agents use LangGraph\"\u001b[39m\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await queryAnalyzerWithExamples.invoke(\"what's the difference between web voyager and reflection agents? do they use langgraph?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
