// Generated by an automated build step. Do not edit.

export namespace Gemini {

  /**
   * Safety setting, affecting the safety-blocking behavior.
   * 
   * Passing a safety setting for a category changes the allowed probability that
   * content is blocked.
   */
  export interface SafetySetting {
    /** Required. The category for this setting. */
    category: HarmCategory;

    /**
     * Required. Controls the probability threshold at which harm is blocked.
     */
    threshold: "HARM_BLOCK_THRESHOLD_UNSPECIFIED" | "BLOCK_LOW_AND_ABOVE" | "BLOCK_MEDIUM_AND_ABOVE" | "BLOCK_ONLY_HIGH" | "BLOCK_NONE" | "OFF" | (string & {});
  }

  export type HarmCategory = "HARM_CATEGORY_UNSPECIFIED" | "HARM_CATEGORY_DEROGATORY" | "HARM_CATEGORY_TOXICITY" | "HARM_CATEGORY_VIOLENCE" | "HARM_CATEGORY_SEXUAL" | "HARM_CATEGORY_MEDICAL" | "HARM_CATEGORY_DANGEROUS" | "HARM_CATEGORY_HARASSMENT" | "HARM_CATEGORY_HATE_SPEECH" | "HARM_CATEGORY_SEXUALLY_EXPLICIT" | "HARM_CATEGORY_DANGEROUS_CONTENT" | "HARM_CATEGORY_CIVIC_INTEGRITY" | (string & {});

  /** Config for thinking features. */
  export interface ThinkingConfig {
    /**
     * Indicates whether to include thoughts in the response.
     * If true, thoughts are returned only when available.
     */
    includeThoughts?: boolean;

    /**
     * The number of thoughts tokens that the model should generate.
     */
    thinkingBudget?: number;

    /**
     * Optional. Controls the maximum depth of the model's internal reasoning process before
     * it produces a response. If not specified, the default is HIGH. Recommended
     * for Gemini 3 or later models. Use with earlier models results in an error.
     */
    thinkingLevel?: "THINKING_LEVEL_UNSPECIFIED" | "MINIMAL" | "LOW" | "MEDIUM" | "HIGH" | (string & {});
  }

  export type GenerativeLanguageModality = "MODALITY_UNSPECIFIED" | "TEXT" | "IMAGE" | "VIDEO" | "AUDIO" | "DOCUMENT" | (string & {});

  /** Media resolution for the input media. */
  export interface MediaResolution {
    level?: "MEDIA_RESOLUTION_UNSPECIFIED" | "MEDIA_RESOLUTION_LOW" | "MEDIA_RESOLUTION_MEDIUM" | "MEDIA_RESOLUTION_HIGH" | "MEDIA_RESOLUTION_ULTRA_HIGH" | (string & {});
  }

  /** Config for speech generation and transcription. */
  export interface SpeechConfig {
    /**
     * Optional. The IETF [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language
     * code that the user configured the app to use. Used for speech recognition
     * and synthesis.
     * 
     * Valid values are: `de-DE`, `en-AU`, `en-GB`, `en-IN`, `en-US`, `es-US`,
     * `fr-FR`, `hi-IN`, `pt-BR`, `ar-XA`, `es-ES`, `fr-CA`, `id-ID`, `it-IT`,
     * `ja-JP`, `tr-TR`, `vi-VN`, `bn-IN`, `gu-IN`, `kn-IN`, `ml-IN`, `mr-IN`,
     * `ta-IN`, `te-IN`, `nl-NL`, `ko-KR`, `cmn-CN`, `pl-PL`, `ru-RU`, and
     * `th-TH`.
     */
    languageCode?: string;

    /**
     * Optional. The configuration for the multi-speaker setup.
     * It is mutually exclusive with the voice_config field.
     */
    multiSpeakerVoiceConfig?: MultiSpeakerVoiceConfig;

    /** The configuration in case of single-voice output. */
    voiceConfig?: VoiceConfig;
  }

  /** The configuration for the voice to use. */
  export interface VoiceConfig {
    /** The configuration for the prebuilt voice to use. */
    prebuiltVoiceConfig?: PrebuiltVoiceConfig;
  }

  /** The configuration for the multi-speaker setup. */
  export interface MultiSpeakerVoiceConfig {
    /** Required. All the enabled speaker voices. */
    speakerVoiceConfigs: Array<SpeakerVoiceConfig>;
  }

  /**
   * The configuration for a single speaker in a multi speaker setup.
   */
  export interface SpeakerVoiceConfig {
    /**
     * Required. The name of the speaker to use. Should be the same as in the prompt.
     */
    speaker: string;

    /** Required. The configuration for the voice to use. */
    voiceConfig: VoiceConfig;
  }

  /** Config for image generation features. */
  export interface ImageConfig {
    /**
     * Optional. The aspect ratio of the image to generate. Supported aspect ratios: `1:1`,
     * `2:3`, `3:2`, `3:4`, `4:3`, `4:5`, `5:4`, `9:16`, `16:9`, or `21:9`.
     * 
     * If not specified, the model will choose a default aspect ratio based on any
     * reference images provided.
     */
    aspectRatio?: string;

    /**
     * Optional. Specifies the size of generated images. Supported values are `1K`, `2K`,
     * `4K`. If not specified, the model will use default value `1K`.
     */
    imageSize?: string;
  }

  /**
   * A datatype containing media that is part of a multi-part `Content` message.
   * 
   * A `Part` consists of data which has an associated datatype. A `Part` can only
   * contain one of the accepted types in `Part.data`.
   * 
   * A `Part` must have a fixed IANA MIME type identifying the type and subtype
   * of the media if the `inline_data` field is filled with raw bytes.
   */
  export interface Part {
    /** Result of executing the `ExecutableCode`. */
    codeExecutionResult?: CodeExecutionResult;

    /** Code generated by the model that is meant to be executed. */
    executableCode?: ExecutableCode;

    /** URI based data. */
    fileData?: FileData;

    /**
     * A predicted `FunctionCall` returned from the model that contains
     * a string representing the `FunctionDeclaration.name` with the
     * arguments and their values.
     */
    functionCall?: FunctionCall;

    /**
     * The result output of a `FunctionCall` that contains a string
     * representing the `FunctionDeclaration.name` and a structured JSON
     * object containing any output from the function is used as context to
     * the model.
     */
    functionResponse?: FunctionResponse;

    /** Inline media bytes. */
    inlineData?: Blob;

    /** Optional. Media resolution for the input media. */
    mediaResolution?: MediaResolution;

    /**
     * Custom metadata associated with the Part.
     * Agents using genai.Part as content representation may need to keep track
     * of the additional information. For example it can be name of a file/source
     * from which the Part originates or a way to multiplex multiple Part streams.
     */
    partMetadata?: Record<string, unknown>;

    /** Inline text. */
    text?: string;

    /** Optional. Indicates if the part is thought from the model. */
    thought?: boolean;

    /**
     * Optional. An opaque signature for the thought so it can be reused in subsequent
     * requests.
     */
    thoughtSignature?: string;

    /**
     * Optional. Video metadata. The metadata should only be specified while the video
     * data is presented in inline_data or file_data.
     */
    videoMetadata?: VideoMetadata;
  }

  /** Metadata describes the input video content. */
  export interface VideoMetadata {
    /** Optional. The end offset of the video. */
    endOffset?: string;

    /**
     * Optional. The frame rate of the video sent to the model. If not specified, the
     * default value will be 1.0.
     * The fps range is (0.0, 24.0].
     */
    fps?: number;

    /** Optional. The start offset of the video. */
    startOffset?: string;
  }

  /**
   * Result of executing the `ExecutableCode`.
   * 
   * Only generated when using the `CodeExecution`, and always follows a `part`
   * containing the `ExecutableCode`.
   */
  export interface CodeExecutionResult {
    /** Required. Outcome of the code execution. */
    outcome: "OUTCOME_UNSPECIFIED" | "OUTCOME_OK" | "OUTCOME_FAILED" | "OUTCOME_DEADLINE_EXCEEDED" | (string & {});

    /**
     * Optional. Contains stdout when code execution is successful, stderr or other
     * description otherwise.
     */
    output?: string;
  }

  /**
   * Code generated by the model that is meant to be executed, and the result
   * returned to the model.
   * 
   * Only generated when using the `CodeExecution` tool, in which the code will
   * be automatically executed, and a corresponding `CodeExecutionResult` will
   * also be generated.
   */
  export interface ExecutableCode {
    /** Required. The code to be executed. */
    code: string;

    /** Required. Programming language of the `code`. */
    language: "LANGUAGE_UNSPECIFIED" | "PYTHON" | (string & {});
  }

  /**
   * The base structured datatype containing multi-part content of a message.
   * 
   * A `Content` includes a `role` field designating the producer of the `Content`
   * and a `parts` field containing multi-part data that contains the content of
   * the message turn.
   */
  export interface Content {
    /**
     * Ordered `Parts` that constitute a single message. Parts may have different
     * MIME types.
     */
    parts?: Array<Part>;

    /**
     * Optional. The producer of the content. Must be either 'user' or 'model'.
     * 
     * Useful to set for multi-turn conversations, otherwise can be left blank
     * or unset.
     */
    role?: string;
  }

  /**
   * Configuration options for model generation and outputs. Not all parameters
   * are configurable for every model.
   */
  export interface GenerationConfig {
    /**
     * Optional. Output schema of the generated response. This is an alternative to
     * `response_schema` that accepts [JSON Schema](https://json-schema.org/).
     * 
     * If set, `response_schema` must be omitted, but `response_mime_type` is
     * required.
     * 
     * While the full JSON Schema may be sent, not all features are supported.
     * Specifically, only the following properties are supported:
     * 
     * - `$id`
     * - `$defs`
     * - `$ref`
     * - `$anchor`
     * - `type`
     * - `format`
     * - `title`
     * - `description`
     * - `enum` (for strings and numbers)
     * - `items`
     * - `prefixItems`
     * - `minItems`
     * - `maxItems`
     * - `minimum`
     * - `maximum`
     * - `anyOf`
     * - `oneOf` (interpreted the same as `anyOf`)
     * - `properties`
     * - `additionalProperties`
     * - `required`
     * 
     * The non-standard `propertyOrdering` property may also be set.
     * 
     * Cyclic references are unrolled to a limited degree and, as such, may only
     * be used within non-required properties. (Nullable properties are not
     * sufficient.) If `$ref` is set on a sub-schema, no other properties, except
     * for than those starting as a `$`, may be set.
     */
    _responseJsonSchema?: unknown;

    /**
     * Optional. Number of generated responses to return. If unset, this will default
     * to 1. Please note that this doesn't work for previous generation
     * models (Gemini 1.0 family)
     */
    candidateCount?: number;

    /**
     * Optional. Enables enhanced civic answers. It may not be available for all models.
     */
    enableEnhancedCivicAnswers?: boolean;

    /**
     * Optional. Frequency penalty applied to the next token's logprobs, multiplied by the
     * number of times each token has been seen in the respponse so far.
     * 
     * A positive penalty will discourage the use of tokens that have already
     * been used, proportional to the number of times the token has been used:
     * The more a token is used, the more difficult it is for the model to use
     * that token again increasing the vocabulary of responses.
     * 
     * Caution: A _negative_ penalty will encourage the model to reuse tokens
     * proportional to the number of times the token has been used. Small
     * negative values will reduce the vocabulary of a response. Larger negative
     * values will cause the model to start repeating a common token  until it
     * hits the max_output_tokens
     * limit.
     */
    frequencyPenalty?: number;

    /**
     * Optional. Config for image generation.
     * An error will be returned if this field is set for models that don't
     * support these config options.
     */
    imageConfig?: ImageConfig;

    /**
     * Optional. Only valid if response_logprobs=True.
     * This sets the number of top logprobs to return at each decoding step in the
     * Candidate.logprobs_result. The number must be in the range of [0, 20].
     */
    logprobs?: number;

    /**
     * Optional. The maximum number of tokens to include in a response candidate.
     * 
     * Note: The default value varies by model, see the `Model.output_token_limit`
     * attribute of the `Model` returned from the `getModel` function.
     */
    maxOutputTokens?: number;

    /**
     * Optional. If specified, the media resolution specified will be used.
     */
    mediaResolution?: "MEDIA_RESOLUTION_UNSPECIFIED" | "MEDIA_RESOLUTION_LOW" | "MEDIA_RESOLUTION_MEDIUM" | "MEDIA_RESOLUTION_HIGH" | (string & {});

    /**
     * Optional. Presence penalty applied to the next token's logprobs if the token has
     * already been seen in the response.
     * 
     * This penalty is binary on/off and not dependant on the number of times the
     * token is used (after the first). Use
     * frequency_penalty
     * for a penalty that increases with each use.
     * 
     * A positive penalty will discourage the use of tokens that have already
     * been used in the response, increasing the vocabulary.
     * 
     * A negative penalty will encourage the use of tokens that have already been
     * used in the response, decreasing the vocabulary.
     */
    presencePenalty?: number;

    /**
     * Optional. An internal detail. Use `responseJsonSchema` rather than this field.
     */
    responseJsonSchema?: unknown;

    /** Optional. If true, export the logprobs results in response. */
    responseLogprobs?: boolean;

    /**
     * Optional. MIME type of the generated candidate text.
     * Supported MIME types are:
     * `text/plain`: (default) Text output.
     * `application/json`: JSON response in the response candidates.
     * `text/x.enum`: ENUM as a string response in the response candidates.
     * Refer to the
     * [docs](https://ai.google.dev/gemini-api/docs/prompting_with_media#plain_text_formats)
     * for a list of all supported text MIME types.
     */
    responseMimeType?: string;

    /**
     * Optional. The requested modalities of the response. Represents the set of modalities
     * that the model can return, and should be expected in the response. This is
     * an exact match to the modalities of the response.
     * 
     * A model may have multiple combinations of supported modalities. If the
     * requested modalities do not match any of the supported combinations, an
     * error will be returned.
     * 
     * An empty list is equivalent to requesting only text.
     */
    responseModalities?: Array<"MODALITY_UNSPECIFIED" | "TEXT" | "IMAGE" | "AUDIO" | (string & {})>;

    /**
     * Optional. Output schema of the generated candidate text. Schemas must be a
     * subset of the [OpenAPI schema](https://spec.openapis.org/oas/v3.0.3#schema)
     * and can be objects, primitives or arrays.
     * 
     * If set, a compatible `response_mime_type` must also be set.
     * Compatible MIME types:
     * `application/json`: Schema for JSON response.
     * Refer to the [JSON text generation
     * guide](https://ai.google.dev/gemini-api/docs/json-mode) for more details.
     */
    responseSchema?: Gemini.Tools.Schema;

    /**
     * Optional. Seed used in decoding. If not set, the request uses a randomly generated
     * seed.
     */
    seed?: number;

    /** Optional. The speech generation config. */
    speechConfig?: SpeechConfig;

    /**
     * Optional. The set of character sequences (up to 5) that will stop output generation.
     * If specified, the API will stop at the first appearance of a
     * `stop_sequence`. The stop sequence will not be included as part of the
     * response.
     */
    stopSequences?: Array<string>;

    /**
     * Optional. Controls the randomness of the output.
     * 
     * Note: The default value varies by model, see the `Model.temperature`
     * attribute of the `Model` returned from the `getModel` function.
     * 
     * Values can range from [0.0, 2.0].
     */
    temperature?: number;

    /**
     * Optional. Config for thinking features.
     * An error will be returned if this field is set for models that don't
     * support thinking.
     */
    thinkingConfig?: ThinkingConfig;

    /**
     * Optional. The maximum number of tokens to consider when sampling.
     * 
     * Gemini models use Top-p (nucleus) sampling or a combination of Top-k and
     * nucleus sampling. Top-k sampling considers the set of `top_k` most probable
     * tokens. Models running with nucleus sampling don't allow top_k setting.
     * 
     * Note: The default value varies by `Model` and is specified by
     * the`Model.top_p` attribute returned from the `getModel` function. An empty
     * `top_k` attribute indicates that the model doesn't apply top-k sampling
     * and doesn't allow setting `top_k` on requests.
     */
    topK?: number;

    /**
     * Optional. The maximum cumulative probability of tokens to consider when sampling.
     * 
     * The model uses combined Top-k and Top-p (nucleus) sampling.
     * 
     * Tokens are sorted based on their assigned probabilities so that only the
     * most likely tokens are considered. Top-k sampling directly limits the
     * maximum number of tokens to consider, while Nucleus sampling limits the
     * number of tokens based on the cumulative probability.
     * 
     * Note: The default value varies by `Model` and is specified by
     * the`Model.top_p` attribute returned from the `getModel` function. An empty
     * `top_k` attribute indicates that the model doesn't apply top-k sampling
     * and doesn't allow setting `top_k` on requests.
     */
    topP?: number;
  }

  export namespace Tools {

    /**
     * Structured representation of a function declaration as defined by the
     * [OpenAPI 3.03 specification](https://spec.openapis.org/oas/v3.0.3). Included
     * in this declaration are the function name and parameters. This
     * FunctionDeclaration is a representation of a block of code that can be used
     * as a `Tool` by the model and executed by the client.
     */
    export interface FunctionDeclaration {
      /**
       * Optional. Specifies the function Behavior.
       * Currently only supported by the BidiGenerateContent method.
       */
      behavior?: "UNSPECIFIED" | "BLOCKING" | "NON_BLOCKING" | (string & {});

      /** Required. A brief description of the function. */
      description: string;

      /**
       * Required. The name of the function.
       * Must be a-z, A-Z, 0-9, or contain underscores, colons, dots, and dashes,
       * with a maximum length of 64.
       */
      name: string;

      /**
       * Optional. Describes the parameters to this function. Reflects the Open API 3.03
       * Parameter Object string Key: the name of the parameter. Parameter names are
       * case sensitive. Schema Value: the Schema defining the type used for the
       * parameter.
       */
      parameters?: Schema;

      /**
       * Optional. Describes the parameters to the function in JSON Schema format. The schema
       * must describe an object where the properties are the parameters to the
       * function. For example:
       * 
       * ```
       * {
       *   "type": "object",
       *   "properties": {
       *     "name": { "type": "string" },
       *     "age": { "type": "integer" }
       *   },
       *   "additionalProperties": false,
       *   "required": ["name", "age"],
       *   "propertyOrdering": ["name", "age"]
       * }
       * ```
       * 
       * This field is mutually exclusive with `parameters`.
       */
      parametersJsonSchema?: unknown;

      /**
       * Optional. Describes the output from this function in JSON Schema format. Reflects the
       * Open API 3.03 Response Object. The Schema defines the type used for the
       * response value of the function.
       */
      response?: Schema;

      /**
       * Optional. Describes the output from this function in JSON Schema format. The value
       * specified by the schema is the response value of the function.
       * 
       * This field is mutually exclusive with `response`.
       */
      responseJsonSchema?: unknown;
    }

    /**
     * GoogleSearch tool type.
     * Tool to support Google Search in Model. Powered by Google.
     */
    export interface GoogleSearch {
      /**
       * Optional. Filter search results to a specific time range.
       * If customers set a start time, they must set an end time (and vice
       * versa).
       */
      timeRangeFilter?: Interval;
    }

    /**
     * Tool that executes code generated by the model, and automatically returns
     * the result to the model.
     * 
     * See also `ExecutableCode` and `CodeExecutionResult` which are only generated
     * when using this tool.
     */
    export interface CodeExecution {
    }

    /** Tool to support URL context retrieval. */
    export interface UrlContext {
    }

    /**
     * The GoogleMaps Tool that provides geospatial context for the user's query.
     */
    export interface GoogleMaps {
      /**
       * Optional. Whether to return a widget context token in the GroundingMetadata of the
       * response. Developers can use the widget context token to render a Google
       * Maps widget with geospatial context related to the places that the model
       * references in the response.
       */
      enableWidget?: boolean;
    }

    /**
     * The FileSearch tool that retrieves knowledge from Semantic Retrieval corpora.
     * Files are imported to Semantic Retrieval corpora using the ImportFile API.
     */
    export interface FileSearch {
      /**
       * Required. The names of the file_search_stores to retrieve from.
       * Example: `fileSearchStores/my-file-search-store-123`
       */
      fileSearchStoreNames: Array<string>;

      /**
       * Optional. Metadata filter to apply to the semantic retrieval documents and chunks.
       */
      metadataFilter?: string;

      /**
       * Optional. The number of semantic retrieval chunks to retrieve.
       */
      topK?: number;
    }

    /** Computer Use tool type. */
    export interface ComputerUse {
      /** Required. The environment being operated. */
      environment: "ENVIRONMENT_UNSPECIFIED" | "ENVIRONMENT_BROWSER" | (string & {});

      /**
       * Optional. By default, predefined functions are included in the final model
       * call.
       * Some of them can be explicitly excluded from being automatically
       * included. This can serve two purposes:
       * 1. Using a more restricted / different action space.
       * 2. Improving the definitions / instructions of predefined functions.
       */
      excludedPredefinedFunctions?: Array<string>;
    }

    /** Configuration for specifying function calling behavior. */
    export interface FunctionCallingConfig {
      /**
       * Optional. A set of function names that, when provided, limits the functions the model
       * will call.
       * 
       * This should only be set when the Mode is ANY or VALIDATED. Function names
       * should match [FunctionDeclaration.name]. When set, model will
       * predict a function call from only allowed function names.
       */
      allowedFunctionNames?: Array<string>;

      /**
       * Optional. Specifies the mode in which function calling should execute. If
       * unspecified, the default value will be set to AUTO.
       */
      mode?: "MODE_UNSPECIFIED" | "AUTO" | "ANY" | "NONE" | "VALIDATED" | (string & {});
    }

    /** Retrieval config. */
    export interface RetrievalConfig {
      /**
       * Optional. The language code of the user.
       * Language code for content. Use language tags defined by
       * [BCP47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt).
       */
      languageCode?: string;

      /** Optional. The location of the user. */
      latLng?: LatLng;
    }

    /**
     * The Tool configuration containing parameters for specifying `Tool` use
     * in the request.
     */
    export interface ToolConfig {
      /** Optional. Function calling config. */
      functionCallingConfig?: FunctionCallingConfig;

      /** Optional. Retrieval config. */
      retrievalConfig?: RetrievalConfig;
    }

    /**
     * The `Schema` object allows the definition of input and output data types.
     * These types can be objects, but also primitives and arrays.
     * Represents a select subset of an [OpenAPI 3.0 schema
     * object](https://spec.openapis.org/oas/v3.0.3#schema).
     */
    export interface Schema {
      /**
       * Optional. The value should be validated against any (one or more) of the subschemas
       * in the list.
       */
      anyOf?: Array<Schema>;

      /**
       * Optional. Default value of the field. Per JSON Schema, this field is intended for
       * documentation generators and doesn't affect validation. Thus it's included
       * here and ignored so that developers who send schemas with a `default` field
       * don't get unknown-field errors.
       */
      default?: unknown;

      /**
       * Optional. A brief description of the parameter. This could contain examples of use.
       * Parameter description may be formatted as Markdown.
       */
      description?: string;

      /**
       * Optional. Possible values of the element of Type.STRING with enum format.
       * For example we can define an Enum Direction as :
       * {type:STRING, format:enum, enum:["EAST", NORTH", "SOUTH", "WEST"]}
       */
      enum?: Array<string>;

      /**
       * Optional. Example of the object. Will only populated when the object is the root.
       */
      example?: unknown;

      /**
       * Optional. The format of the data. Any value is allowed, but most do not trigger any
       * special functionality.
       */
      format?: string;

      /** Optional. Schema of the elements of Type.ARRAY. */
      items?: Schema;

      /** Optional. Maximum number of the elements for Type.ARRAY. */
      maxItems?: string;

      /** Optional. Maximum length of the Type.STRING */
      maxLength?: string;

      /** Optional. Maximum number of the properties for Type.OBJECT. */
      maxProperties?: string;

      /** Optional. Maximum value of the Type.INTEGER and Type.NUMBER */
      maximum?: number;

      /** Optional. Minimum number of the elements for Type.ARRAY. */
      minItems?: string;

      /**
       * Optional. SCHEMA FIELDS FOR TYPE STRING
       * Minimum length of the Type.STRING
       */
      minLength?: string;

      /** Optional. Minimum number of the properties for Type.OBJECT. */
      minProperties?: string;

      /**
       * Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER
       * Minimum value of the Type.INTEGER and Type.NUMBER
       */
      minimum?: number;

      /** Optional. Indicates if the value may be null. */
      nullable?: boolean;

      /**
       * Optional. Pattern of the Type.STRING to restrict a string to a regular expression.
       */
      pattern?: string;

      /** Optional. Properties of Type.OBJECT. */
      properties?: Record<string, unknown>;

      /**
       * Optional. The order of the properties.
       * Not a standard field in open api spec. Used to determine the order of the
       * properties in the response.
       */
      propertyOrdering?: Array<string>;

      /** Optional. Required properties of Type.OBJECT. */
      required?: Array<string>;

      /** Optional. The title of the schema. */
      title?: string;

      /** Required. Data type. */
      type: Type;
    }

    export type Type = "TYPE_UNSPECIFIED" | "STRING" | "NUMBER" | "INTEGER" | "BOOLEAN" | "ARRAY" | "OBJECT" | "NULL" | (string & {});

    /**
     * Represents a time interval, encoded as a Timestamp start (inclusive) and a
     * Timestamp end (exclusive).
     * 
     * The start must be less than or equal to the end.
     * When the start equals the end, the interval is empty (matches no time).
     * When both start and end are unspecified, the interval matches any time.
     */
    export interface Interval {
      /**
       * Optional. Exclusive end of the interval.
       * 
       * If specified, a Timestamp matching this interval will have to be before the
       * end.
       */
      endTime?: string;

      /**
       * Optional. Inclusive start of the interval.
       * 
       * If specified, a Timestamp matching this interval will have to be the same
       * or after the start.
       */
      startTime?: string;
    }

    /**
     * An object that represents a latitude/longitude pair. This is expressed as a
     * pair of doubles to represent degrees latitude and degrees longitude. Unless
     * specified otherwise, this object must conform to the
     * WGS84 standard. Values must be within normalized ranges.
     */
    export interface LatLng {
      /**
       * The latitude in degrees. It must be in the range [-90.0, +90.0].
       */
      latitude?: number;

      /**
       * The longitude in degrees. It must be in the range [-180.0, +180.0].
       */
      longitude?: number;
    }
  }

  /** Request to generate a completion from the model. */
  export interface GenerateContentRequest {
    /**
     * Optional. The name of the content
     * [cached](https://ai.google.dev/gemini-api/docs/caching) to use as context
     * to serve the prediction. Format: `cachedContents/{cachedContent}`
     */
    cachedContent?: string;

    /**
     * Required. The content of the current conversation with the model.
     * 
     * For single-turn queries, this is a single instance. For multi-turn queries
     * like [chat](https://ai.google.dev/gemini-api/docs/text-generation#chat),
     * this is a repeated field that contains the conversation history and the
     * latest request.
     */
    contents: Array<Content>;

    /**
     * Optional. Configuration options for model generation and outputs.
     */
    generationConfig?: GenerationConfig;

    /**
     * Required. The name of the `Model` to use for generating the completion.
     * 
     * Format: `models/{model}`.
     */
    model: string;

    /**
     * Optional. A list of unique `SafetySetting` instances for blocking unsafe content.
     * 
     * This will be enforced on the `GenerateContentRequest.contents` and
     * `GenerateContentResponse.candidates`. There should not be more than one
     * setting for each `SafetyCategory` type. The API will block any contents and
     * responses that fail to meet the thresholds set by these settings. This list
     * overrides the default settings for each `SafetyCategory` specified in the
     * safety_settings. If there is no `SafetySetting` for a given
     * `SafetyCategory` provided in the list, the API will use the default safety
     * setting for that category. Harm categories HARM_CATEGORY_HATE_SPEECH,
     * HARM_CATEGORY_SEXUALLY_EXPLICIT, HARM_CATEGORY_DANGEROUS_CONTENT,
     * HARM_CATEGORY_HARASSMENT, HARM_CATEGORY_CIVIC_INTEGRITY are supported.
     * Refer to the [guide](https://ai.google.dev/gemini-api/docs/safety-settings)
     * for detailed information on available safety settings. Also refer to the
     * [Safety guidance](https://ai.google.dev/gemini-api/docs/safety-guidance) to
     * learn how to incorporate safety considerations in your AI applications.
     */
    safetySettings?: Array<SafetySetting>;

    /**
     * Optional. Developer set [system
     * instruction(s)](https://ai.google.dev/gemini-api/docs/system-instructions).
     * Currently, text only.
     */
    systemInstruction?: Content;

    /**
     * Optional. Tool configuration for any `Tool` specified in the request. Refer to the
     * [Function calling
     * guide](https://ai.google.dev/gemini-api/docs/function-calling#function_calling_mode)
     * for a usage example.
     */
    toolConfig?: Gemini.Tools.ToolConfig;

    /**
     * Optional. A list of `Tools` the `Model` may use to generate the next response.
     * 
     * A `Tool` is a piece of code that enables the system to interact with
     * external systems to perform an action, or set of actions, outside of
     * knowledge and scope of the `Model`. Supported `Tool`s are `Function` and
     * `code_execution`. Refer to the [Function
     * calling](https://ai.google.dev/gemini-api/docs/function-calling) and the
     * [Code execution](https://ai.google.dev/gemini-api/docs/code-execution)
     * guides to learn more.
     */
    tools?: Array<Tool>;
  }

  /**
   * Safety rating for a piece of content.
   * 
   * The safety rating contains the category of harm and the
   * harm probability level in that category for a piece of content.
   * Content is classified for safety across a number of
   * harm categories and the probability of the harm classification is included
   * here.
   */
  export interface SafetyRating {
    /** Was this content blocked because of this rating? */
    blocked?: boolean;

    /** Required. The category for this rating. */
    category: HarmCategory;

    /** Required. The probability of harm for this content. */
    probability: "HARM_PROBABILITY_UNSPECIFIED" | "NEGLIGIBLE" | "LOW" | "MEDIUM" | "HIGH" | (string & {});
  }

  /** A collection of source attributions for a piece of content. */
  export interface CitationMetadata {
    /** Citations to sources for a specific response. */
    citationSources?: Array<CitationSource>;
  }

  /** Metadata returned to client when grounding is enabled. */
  export interface GroundingMetadata {
    /**
     * Optional. Resource name of the Google Maps widget context token that can be used
     * with the PlacesContextElement widget in order to render contextual data.
     * Only populated in the case that grounding with Google Maps is enabled.
     */
    googleMapsWidgetContextToken?: string;

    /**
     * List of supporting references retrieved from specified grounding source.
     * When streaming, this only contains the grounding chunks that have not been
     * included in the grounding metadata of previous responses.
     */
    groundingChunks?: Array<GroundingChunk>;

    /** List of grounding support. */
    groundingSupports?: Array<GoogleAiGenerativelanguageV1betaGroundingSupport>;

    /** Metadata related to retrieval in the grounding flow. */
    retrievalMetadata?: RetrievalMetadata;

    /**
     * Optional. Google search entry for the following-up web searches.
     */
    searchEntryPoint?: SearchEntryPoint;

    /** Web search queries for the following-up web search. */
    webSearchQueries?: Array<string>;
  }

  /** Grounding chunk. */
  export interface GroundingChunk {
    /** Optional. Grounding chunk from Google Maps. */
    maps?: Maps;

    /**
     * Optional. Grounding chunk from context retrieved by the file search tool.
     */
    retrievedContext?: RetrievedContext;

    /** Grounding chunk from the web. */
    web?: Web;
  }

  /** Chunk from context retrieved by the file search tool. */
  export interface RetrievedContext {
    /**
     * Optional. Name of the `FileSearchStore` containing the document.
     * Example: `fileSearchStores/123`
     */
    fileSearchStore?: string;

    /** Optional. Text of the chunk. */
    text?: string;

    /** Optional. Title of the document. */
    title?: string;

    /** Optional. URI reference of the semantic retrieval document. */
    uri?: string;
  }

  /** Grounding support. */
  export interface GoogleAiGenerativelanguageV1betaGroundingSupport {
    /**
     * Optional. Confidence score of the support references. Ranges from 0 to 1. 1 is the
     * most confident. This list must have the same size as the
     * grounding_chunk_indices.
     */
    confidenceScores?: Array<number>;

    /**
     * Optional. A list of indices (into 'grounding_chunk' in
     * `response.candidate.grounding_metadata`) specifying the citations
     * associated with the claim. For instance [1,3,4] means that
     * grounding_chunk[1], grounding_chunk[3], grounding_chunk[4] are the
     * retrieved content attributed to the claim. If the response is streaming,
     * the grounding_chunk_indices refer to the indices across all responses.
     * It is the client's responsibility to accumulate the grounding chunks from
     * all responses (while maintaining the same order).
     */
    groundingChunkIndices?: Array<number>;

    /** Segment of the content this support belongs to. */
    segment?: GoogleAiGenerativelanguageV1betaSegment;
  }

  /** Segment of the content. */
  export interface GoogleAiGenerativelanguageV1betaSegment {
    /**
     * End index in the given Part, measured in bytes. Offset from the start of
     * the Part, exclusive, starting at zero.
     */
    endIndex?: number;

    /** The index of a Part object within its parent Content object. */
    partIndex?: number;

    /**
     * Start index in the given Part, measured in bytes. Offset from the start of
     * the Part, inclusive, starting at zero.
     */
    startIndex?: number;

    /** The text corresponding to the segment from the response. */
    text?: string;
  }

  /** Metadata related to retrieval in the grounding flow. */
  export interface RetrievalMetadata {
    /**
     * Optional. Score indicating how likely information from google search could help
     * answer the prompt. The score is in the range [0, 1], where 0 is the least
     * likely and 1 is the most likely. This score is only populated when
     * google search grounding and dynamic retrieval is enabled. It will be
     * compared to the threshold to determine whether to trigger google search.
     */
    googleSearchDynamicRetrievalScore?: number;
  }

  /** Identifier for a part within a `GroundingPassage`. */
  export interface GroundingPassageId {
    /**
     * Output only. Index of the part within the `GenerateAnswerRequest`'s
     * `GroundingPassage.content`.
     */
    readonly partIndex?: number;

    /**
     * Output only. ID of the passage matching the `GenerateAnswerRequest`'s
     * `GroundingPassage.id`.
     */
    readonly passageId?: string;
  }

  /**
   * Identifier for a `Chunk` retrieved via Semantic Retriever specified in the
   * `GenerateAnswerRequest` using `SemanticRetrieverConfig`.
   */
  export interface SemanticRetrieverChunk {
    /**
     * Output only. Name of the `Chunk` containing the attributed text.
     * Example: `corpora/123/documents/abc/chunks/xyz`
     */
    readonly chunk?: string;

    /**
     * Output only. Name of the source matching the request's
     * `SemanticRetrieverConfig.source`. Example: `corpora/123` or
     * `corpora/123/documents/abc`
     */
    readonly source?: string;
  }

  /** Identifier for the source contributing to this attribution. */
  export interface AttributionSourceId {
    /** Identifier for an inline passage. */
    groundingPassage?: GroundingPassageId;

    /** Identifier for a `Chunk` fetched via Semantic Retriever. */
    semanticRetrieverChunk?: SemanticRetrieverChunk;
  }

  /** Attribution for a source that contributed to an answer. */
  export interface GroundingAttribution {
    /** Grounding source content that makes up this attribution. */
    content?: Content;

    /**
     * Output only. Identifier for the source contributing to this attribution.
     */
    readonly sourceId?: AttributionSourceId;
  }

  /** Context of the a single url retrieval. */
  export interface UrlMetadata {
    /** Retrieved url by the tool. */
    retrievedUrl?: string;

    /** Status of the url retrieval. */
    urlRetrievalStatus?: "URL_RETRIEVAL_STATUS_UNSPECIFIED" | "URL_RETRIEVAL_STATUS_SUCCESS" | "URL_RETRIEVAL_STATUS_ERROR" | "URL_RETRIEVAL_STATUS_PAYWALL" | "URL_RETRIEVAL_STATUS_UNSAFE" | (string & {});
  }

  /** Metadata related to url context retrieval tool. */
  export interface UrlContextMetadata {
    /** List of url context. */
    urlMetadata?: Array<UrlMetadata>;
  }

  /** Logprobs Result */
  export interface LogprobsResult {
    /**
     * Length = total number of decoding steps.
     * The chosen candidates may or may not be in top_candidates.
     */
    chosenCandidates?: Array<LogprobsResultCandidate>;

    /** Sum of log probabilities for all tokens. */
    logProbabilitySum?: number;

    /** Length = total number of decoding steps. */
    topCandidates?: Array<TopCandidates>;
  }

  /** Candidates with top log probabilities at each decoding step. */
  export interface TopCandidates {
    /** Sorted by log probability in descending order. */
    candidates?: Array<LogprobsResultCandidate>;
  }

  /** Candidate for the logprobs token and score. */
  export interface LogprobsResultCandidate {
    /** The candidate's log probability. */
    logProbability?: number;

    /** The candidate’s token string value. */
    token?: string;

    /** The candidate’s token id value. */
    tokenId?: number;
  }

  /** A response candidate generated from the model. */
  export interface Candidate {
    /** Output only. Average log probability score of the candidate. */
    readonly avgLogprobs?: number;

    /**
     * Output only. Citation information for model-generated candidate.
     * 
     * This field may be populated with recitation information for any text
     * included in the `content`. These are passages that are "recited" from
     * copyrighted material in the foundational LLM's training data.
     */
    readonly citationMetadata?: CitationMetadata;

    /** Output only. Generated content returned from the model. */
    readonly content?: Content;

    /**
     * Optional. Output only. Details the reason why the model stopped generating tokens.
     * This is populated only when `finish_reason` is set.
     */
    readonly finishMessage?: string;

    /**
     * Optional. Output only. The reason why the model stopped generating tokens.
     * 
     * If empty, the model has not stopped generating tokens.
     */
    readonly finishReason?: "FINISH_REASON_UNSPECIFIED" | "STOP" | "MAX_TOKENS" | "SAFETY" | "RECITATION" | "LANGUAGE" | "OTHER" | "BLOCKLIST" | "PROHIBITED_CONTENT" | "SPII" | "MALFORMED_FUNCTION_CALL" | "IMAGE_SAFETY" | "IMAGE_PROHIBITED_CONTENT" | "IMAGE_OTHER" | "NO_IMAGE" | "IMAGE_RECITATION" | "UNEXPECTED_TOOL_CALL" | "TOO_MANY_TOOL_CALLS" | "MISSING_THOUGHT_SIGNATURE" | "MALFORMED_RESPONSE" | (string & {});

    /**
     * Output only. Attribution information for sources that contributed to a grounded answer.
     * 
     * This field is populated for `GenerateAnswer` calls.
     */
    readonly groundingAttributions?: Array<GroundingAttribution>;

    /**
     * Output only. Grounding metadata for the candidate.
     * 
     * This field is populated for `GenerateContent` calls.
     */
    readonly groundingMetadata?: GroundingMetadata;

    /**
     * Output only. Index of the candidate in the list of response candidates.
     */
    readonly index?: number;

    /**
     * Output only. Log-likelihood scores for the response tokens and top tokens
     */
    readonly logprobsResult?: LogprobsResult;

    /**
     * List of ratings for the safety of a response candidate.
     * 
     * There is at most one rating per category.
     */
    safetyRatings?: Array<SafetyRating>;

    /** Output only. Token count for this candidate. */
    readonly tokenCount?: number;

    /** Output only. Metadata related to url context retrieval tool. */
    readonly urlContextMetadata?: UrlContextMetadata;
  }

  /** Represents token counting info for a single modality. */
  export interface ModalityTokenCount {
    /** The modality associated with this token count. */
    modality?: GenerativeLanguageModality;

    /** Number of tokens. */
    tokenCount?: number;
  }

  /** Metadata on the generation request's token usage. */
  export interface UsageMetadata {
    /**
     * Output only. List of modalities of the cached content in the request input.
     */
    readonly cacheTokensDetails?: Array<ModalityTokenCount>;

    /**
     * Number of tokens in the cached part of the prompt (the cached content)
     */
    cachedContentTokenCount?: number;

    /**
     * Total number of tokens across all the generated response candidates.
     */
    candidatesTokenCount?: number;

    /**
     * Output only. List of modalities that were returned in the response.
     */
    readonly candidatesTokensDetails?: Array<ModalityTokenCount>;

    /**
     * Number of tokens in the prompt. When `cached_content` is set, this is
     * still the total effective prompt size meaning this includes the number of
     * tokens in the cached content.
     */
    promptTokenCount?: number;

    /**
     * Output only. List of modalities that were processed in the request input.
     */
    readonly promptTokensDetails?: Array<ModalityTokenCount>;

    /**
     * Output only. Number of tokens of thoughts for thinking models.
     */
    readonly thoughtsTokenCount?: number;

    /** Output only. Number of tokens present in tool-use prompt(s). */
    readonly toolUsePromptTokenCount?: number;

    /**
     * Output only. List of modalities that were processed for tool-use request inputs.
     */
    readonly toolUsePromptTokensDetails?: Array<ModalityTokenCount>;

    /**
     * Total token count for the generation request (prompt + response
     * candidates).
     */
    totalTokenCount?: number;
  }

  /**
   * A set of the feedback metadata the prompt specified in
   * `GenerateContentRequest.content`.
   */
  export interface PromptFeedback {
    /**
     * Optional. If set, the prompt was blocked and no candidates are returned.
     * Rephrase the prompt.
     */
    blockReason?: "BLOCK_REASON_UNSPECIFIED" | "SAFETY" | "OTHER" | "BLOCKLIST" | "PROHIBITED_CONTENT" | "IMAGE_SAFETY" | (string & {});

    /**
     * Ratings for safety of the prompt.
     * There is at most one rating per category.
     */
    safetyRatings?: Array<SafetyRating>;
  }

  /**
   * Response from the model supporting multiple candidate responses.
   * 
   * Safety ratings and content filtering are reported for both
   * prompt in `GenerateContentResponse.prompt_feedback` and for each candidate
   * in `finish_reason` and in `safety_ratings`. The API:
   *  - Returns either all requested candidates or none of them
   *  - Returns no candidates at all only if there was something wrong with the
   *    prompt (check `prompt_feedback`)
   *  - Reports feedback on each candidate in `finish_reason` and
   *    `safety_ratings`.
   */
  export interface GenerateContentResponse {
    /** Candidate responses from the model. */
    candidates?: Array<Candidate>;

    /** Output only. The current model status of this model. */
    readonly modelStatus?: ModelStatus;

    /**
     * Output only. The model version used to generate the response.
     */
    readonly modelVersion?: string;

    /**
     * Returns the prompt's feedback related to the content filters.
     */
    promptFeedback?: PromptFeedback;

    /** Output only. response_id is used to identify each response. */
    readonly responseId?: string;

    /**
     * Output only. Metadata on the generation requests' token usage.
     */
    readonly usageMetadata?: UsageMetadata;
  }

  /**
   * A grounding chunk from Google Maps. A Maps chunk corresponds to a single
   * place.
   */
  export interface Maps {
    /**
     * Sources that provide answers about the features of a given place in
     * Google Maps.
     */
    placeAnswerSources?: PlaceAnswerSources;

    /**
     * This ID of the place, in `places/{place_id}` format. A user can use this
     * ID to look up that place.
     */
    placeId?: string;

    /** Text description of the place answer. */
    text?: string;

    /** Title of the place. */
    title?: string;

    /** URI reference of the place. */
    uri?: string;
  }

  /** The configuration for the prebuilt speaker to use. */
  export interface PrebuiltVoiceConfig {
    /** The name of the preset voice to use. */
    voiceName?: string;
  }

  /**
   * A predicted `FunctionCall` returned from the model that contains
   * a string representing the `FunctionDeclaration.name` with the
   * arguments and their values.
   */
  export interface FunctionCall {
    /**
     * Optional. The function parameters and values in JSON object format.
     */
    args?: Record<string, unknown>;

    /**
     * Optional. The unique identifier of the function call. If populated, the client to
     * execute the `function_call` and return the response with the matching `id`.
     */
    id?: string;

    /**
     * Required. The name of the function to call.
     * Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum
     * length of 64.
     */
    name: string;
  }

  /**
   * The result output from a `FunctionCall` that contains a string
   * representing the `FunctionDeclaration.name` and a structured JSON
   * object containing any output from the function is used as context to
   * the model. This should contain the result of a`FunctionCall` made
   * based on model prediction.
   */
  export interface FunctionResponse {
    /**
     * Optional. The identifier of the function call this response is for. Populated by the
     * client to match the corresponding function call `id`.
     */
    id?: string;

    /**
     * Required. The name of the function to call.
     * Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum
     * length of 64.
     */
    name: string;

    /**
     * Optional. Ordered `Parts` that constitute a function response. Parts may have
     * different IANA MIME types.
     */
    parts?: Array<FunctionResponsePart>;

    /**
     * Required. The function response in JSON object format.
     * Callers can use any keys of their choice that fit the function's syntax
     * to return the function output, e.g. "output", "result", etc.
     * In particular, if the function call failed to execute, the response can
     * have an "error" key to return error details to the model.
     */
    response: Record<string, unknown>;

    /**
     * Optional. Specifies how the response should be scheduled in the conversation.
     * Only applicable to NON_BLOCKING function calls, is ignored otherwise.
     * Defaults to WHEN_IDLE.
     */
    scheduling?: "SCHEDULING_UNSPECIFIED" | "SILENT" | "WHEN_IDLE" | "INTERRUPT" | (string & {});

    /**
     * Optional. Signals that function call continues, and more responses will be
     * returned, turning the function call into a generator.
     * Is only applicable to NON_BLOCKING function calls, is ignored otherwise.
     * If set to false, future responses will not be considered.
     * It is allowed to return empty `response` with `will_continue=False` to
     * signal that the function call is finished. This may still trigger the model
     * generation. To avoid triggering the generation and finish the function
     * call, additionally set `scheduling` to `SILENT`.
     */
    willContinue?: boolean;
  }

  /**
   * Tool details that the model may use to generate response.
   * 
   * A `Tool` is a piece of code that enables the system to interact with
   * external systems to perform an action, or set of actions, outside of
   * knowledge and scope of the model.
   * 
   * Next ID: 15
   */
  export interface Tool {
    /**
     * Optional. Enables the model to execute code as part of generation.
     */
    codeExecution?: Gemini.Tools.CodeExecution;

    /**
     * Optional. Tool to support the model interacting directly with the computer.
     * If enabled, it automatically populates computer-use specific Function
     * Declarations.
     */
    computerUse?: Gemini.Tools.ComputerUse;

    /**
     * Optional. FileSearch tool type.
     * Tool to retrieve knowledge from Semantic Retrieval corpora.
     */
    fileSearch?: Gemini.Tools.FileSearch;

    /**
     * Optional. A list of `FunctionDeclarations` available to the model that can be used
     * for function calling.
     * 
     * The model or system does not execute the function. Instead the defined
     * function may be returned as a FunctionCall
     * with arguments to the client side for execution. The model may decide to
     * call a subset of these functions by populating
     * FunctionCall in the response. The next
     * conversation turn may contain a
     * FunctionResponse
     * with the Content.role "function" generation context for the next model
     * turn.
     */
    functionDeclarations?: Array<Gemini.Tools.FunctionDeclaration>;

    /**
     * Optional. Tool that allows grounding the model's response with geospatial context
     * related to the user's query.
     */
    googleMaps?: Gemini.Tools.GoogleMaps;

    /**
     * Optional. GoogleSearch tool type.
     * Tool to support Google Search in Model. Powered by Google.
     */
    googleSearch?: Gemini.Tools.GoogleSearch;

    /** Optional. Retrieval tool that is powered by Google search. */
    googleSearchRetrieval?: GoogleSearchRetrieval;

    /** Optional. MCP Servers to connect to. */
    mcpServers?: Array<McpServer>;

    /** Optional. Tool to support URL context retrieval. */
    urlContext?: Gemini.Tools.UrlContext;
  }

  /** Chunk from the web. */
  export interface Web {
    /** Title of the chunk. */
    title?: string;

    /** URI reference of the chunk. */
    uri?: string;
  }

  /** A citation to a source for a portion of a specific response. */
  export interface CitationSource {
    /** Optional. End of the attributed segment, exclusive. */
    endIndex?: number;

    /**
     * Optional. License for the GitHub project that is attributed as a source for segment.
     * 
     * License info is required for code citations.
     */
    license?: string;

    /**
     * Optional. Start of segment of the response that is attributed to this source.
     * 
     * Index indicates the start of the segment, measured in bytes.
     */
    startIndex?: number;

    /**
     * Optional. URI that is attributed as a source for a portion of the text.
     */
    uri?: string;
  }

  /**
   * The status of the underlying model. This is used to indicate the stage of the
   * underlying model and the retirement time if applicable.
   */
  export interface ModelStatus {
    /** A message explaining the model status. */
    message?: string;

    /** The stage of the underlying model. */
    modelStage?: ModelStage;

    /** The time at which the model will be retired. */
    retirementTime?: string;
  }

  /** URI based data. */
  export interface FileData {
    /** Required. URI. */
    fileUri: string;

    /** Optional. The IANA standard MIME type of the source data. */
    mimeType?: string;
  }

  /**
   * Raw media bytes.
   * 
   * Text should not be sent as raw bytes, use the 'text' field.
   */
  export interface Blob {
    /** Raw bytes for media formats. */
    data?: string;

    /**
     * The IANA standard MIME type of the source data.
     * Examples:
     *   - image/png
     *   - image/jpeg
     * If an unsupported MIME type is provided, an error will be returned. For a
     * complete list of supported types, see [Supported file
     * formats](https://ai.google.dev/gemini-api/docs/prompting_with_media#supported_file_formats).
     */
    mimeType?: string;
  }

  /** Google search entry point. */
  export interface SearchEntryPoint {
    /**
     * Optional. Web content snippet that can be embedded in a web page or an app webview.
     */
    renderedContent?: string;

    /** Optional. Base64 encoded JSON representing array of  tuple. */
    sdkBlob?: string;
  }

  /**
   * A MCPServer is a server that can be called by the model to perform actions.
   * It is a server that implements the MCP protocol.
   * Next ID: 5
   */
  export interface McpServer {
    /** The name of the MCPServer. */
    name?: string;

    /** A transport that can stream HTTP requests and responses. */
    streamableHttpTransport?: StreamableHttpTransport;
  }

  export type ModelStage = "MODEL_STAGE_UNSPECIFIED" | "UNSTABLE_EXPERIMENTAL" | "EXPERIMENTAL" | "PREVIEW" | "STABLE" | "LEGACY" | "DEPRECATED" | "RETIRED" | (string & {});

  /**
   * Collection of sources that provide answers about the features of a given
   * place in Google Maps. Each PlaceAnswerSources message corresponds to a
   * specific place in Google Maps. The Google Maps tool used these sources in
   * order to answer questions about features of the place (e.g: "does Bar Foo
   * have Wifi" or "is Foo Bar wheelchair accessible?"). Currently we only
   * support review snippets as sources.
   */
  export interface PlaceAnswerSources {
    /**
     * Snippets of reviews that are used to generate answers about the
     * features of a given place in Google Maps.
     */
    reviewSnippets?: Array<ReviewSnippet>;
  }

  /**
   * A datatype containing media that is part of a `FunctionResponse` message.
   * 
   * A `FunctionResponsePart` consists of data which has an associated datatype. A
   * `FunctionResponsePart` can only contain one of the accepted types in
   * `FunctionResponsePart.data`.
   * 
   * A `FunctionResponsePart` must have a fixed IANA MIME type identifying the
   * type and subtype of the media if the `inline_data` field is filled with raw
   * bytes.
   */
  export interface FunctionResponsePart {
    /** Inline media bytes. */
    inlineData?: FunctionResponseBlob;
  }

  /**
   * Tool to retrieve public web data for grounding, powered by Google.
   */
  export interface GoogleSearchRetrieval {
    /**
     * Specifies the dynamic retrieval configuration for the given source.
     */
    dynamicRetrievalConfig?: DynamicRetrievalConfig;
  }

  /**
   * A transport that can stream HTTP requests and responses.
   * Next ID: 6
   */
  export interface StreamableHttpTransport {
    /**
     * Optional: Fields for authentication headers, timeouts, etc., if needed.
     */
    headers?: Record<string, unknown>;

    /** Timeout for SSE read operations. */
    sseReadTimeout?: string;

    /**
     * Whether to close the client session when the transport closes.
     */
    terminateOnClose?: boolean;

    /** HTTP timeout for regular operations. */
    timeout?: string;

    /**
     * The full URL for the MCPServer endpoint.
     * Example: "https://api.example.com/mcp"
     */
    url?: string;
  }

  /**
   * Encapsulates a snippet of a user review that answers a question about
   * the features of a specific place in Google Maps.
   */
  export interface ReviewSnippet {
    /** A link that corresponds to the user review on Google Maps. */
    googleMapsUri?: string;

    /** The ID of the review snippet. */
    reviewId?: string;

    /** Title of the review. */
    title?: string;
  }

  /**
   * Raw media bytes for function response.
   * 
   * Text should not be sent as raw bytes, use the 'FunctionResponse.response'
   * field.
   */
  export interface FunctionResponseBlob {
    /** Raw bytes for media formats. */
    data?: string;

    /**
     * The IANA standard MIME type of the source data.
     * Examples:
     *   - image/png
     *   - image/jpeg
     * If an unsupported MIME type is provided, an error will be returned. For a
     * complete list of supported types, see [Supported file
     * formats](https://ai.google.dev/gemini-api/docs/prompting_with_media#supported_file_formats).
     */
    mimeType?: string;
  }

  /** Describes the options to customize dynamic retrieval. */
  export interface DynamicRetrievalConfig {
    /**
     * The threshold to be used in dynamic retrieval.
     * If not set, a system default value is used.
     */
    dynamicThreshold?: number;

    /** The mode of the predictor to be used in dynamic retrieval. */
    mode?: "MODE_UNSPECIFIED" | "MODE_DYNAMIC" | (string & {});
  }
}
